{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy as sc\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read individual categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path = './output/' \n",
    "fn = os.listdir(path)\n",
    "fn = [f for f in fn if 'min2.csv' in f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#'/Users/cemgil/src/OpenMaker/Semantics/output/' /Users/hamzazair/Desktop/HamzazProjects/EU\\ Projects/Open\\ Maker/openmaker\\ github/OpenMaker/\n",
    "import os\n",
    "\n",
    "path = './output/' \n",
    "fn = os.listdir(path)\n",
    "\n",
    "DF = []\n",
    "CATEGORIES = []\n",
    "for f in fn:\n",
    "    print(f)\n",
    "    df = pd.read_csv(path+f)\n",
    "    DF.append(df)\n",
    "    #CATEGORIES.append(f.split('_')[1].split('.')[0])\n",
    "    CATEGORIES.append(f[9:].split('.')[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.listdir(\".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for i,x, in enumerate(CATEGORIES):\n",
    "    print(x,i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories_desired_order = [4,0,3,7,6,9,1,8,2,5]\n",
    "for i in categories_desired_order:\n",
    "    print(CATEGORIES[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DF[0].Stem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrieve a news item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "urls = ['https://www.un.org/sg/en/content/sg/speeches/2018-03-28/collective-action-improve-un-peacekeeping-operations-remarks',\n",
    "'https://www.un.org/sg/en/content/sg/speeches/2018-03-26/remembrance-victims-slavery-and-transatlantic-slave-trade-remarks',\n",
    "'https://www.un.org/sg/en/content/sg/speeches/2018-03-23/turtle-bay-security-roundtable-remarks',\n",
    "'https://www.un.org/sg/en/content/sg/speeches/2018-03-22/decade-action-water-sustainable-development-remarks',\n",
    "'https://www.whitehouse.gov/briefings-statements/the-inaugural-address/',\t\n",
    "'http://www.nytimes.com/2009/01/20/us/politics/20text-obama.html',\t\n",
    "'http://www.presidency.ucsb.edu/ws/?pid=25853',\t\n",
    "'http://edition.cnn.com/2001/US/09/11/bush.speech.text/',\n",
    "'http://www.let.rug.nl/usa/documents/1951-/martin-luther-kings-i-have-a-dream-speech-august-28-1963.php']\n",
    "\n",
    "import requests\n",
    "import json\n",
    "\n",
    "insightIP = 'http://178.62.229.16'\n",
    "insightPort = '8484'\n",
    "insightVersion = 'v1.0'\n",
    "apikey = 'thisisinsightapikey'\n",
    "\n",
    "# parameters of news recommendation are twitter_ids, crm_ids\n",
    "\"\"\"\n",
    "twitter_ids = []\n",
    "crm_ids = [135574293]\n",
    "insightSetting = insightIP + ':' + insightPort + '/api/' + insightVersion \n",
    "request = '/recommendation/news?' + 'twitter_ids=' + str(twitter_ids) + '&' + 'crm_ids=' + str(crm_ids)\n",
    "\"\"\"\n",
    "\n",
    "TXT = []\n",
    "\n",
    "for url in urls:\n",
    "    insightSetting = insightIP + ':' + insightPort + '/api/' + insightVersion\n",
    "    request = '/text_analytics/url_scraper?api_key='+apikey+'&url='+url\n",
    "    #res = requests.get(insightSetting + request)\n",
    "    print(insightSetting + request)\n",
    "    res = requests.get(insightSetting + request)\n",
    "    data = res.json()\n",
    "    TXT.append(data['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TXT[9]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Process the input set according to the same procedure as the seed texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with open(\"data/stopwords_standard.txt\", \"r\") as f:\n",
    "    STOP_WORDS_STANDARD = set(f.read().strip().split(\"\\n\"))\n",
    "print(STOP_WORDS_STANDARD)\n",
    "\n",
    "with open(\"data/stopwords_openmaker.txt\", \"r\") as f:\n",
    "    STOP_WORDS_OPENMAKER = set(f.read().strip().split(\"\\n\"))\n",
    "print(STOP_WORDS_OPENMAKER)\n",
    "\n",
    "# merging the two list together\n",
    "STOP_WORDS = STOP_WORDS_STANDARD.union(STOP_WORDS_OPENMAKER)\n",
    "print(STOP_WORDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from utils import tokenizer\n",
    "import nltk\n",
    "from nltk import FreqDist\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from numpy import log, mean\n",
    "import json, csv, re\n",
    "import pprint as pp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokens = nltk.word_tokenize(TXT[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "token_counts = FreqDist(tokens)\n",
    "tokenizer.CHARACTERS_TO_SPLIT += '‘'+'’'+'“'+'”'+'.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for stopword in STOP_WORDS:\n",
    "    if stopword in token_counts:\n",
    "        del token_counts[stopword]\n",
    "        \n",
    "for punctuation in tokenizer.CHARACTERS_TO_SPLIT:\n",
    "    if punctuation in token_counts:\n",
    "        del token_counts[punctuation]\n",
    "\n",
    "        \n",
    "pattern_letters = re.compile('[a-z]')\n",
    "def has_letters(x):\n",
    "    return(pattern_letters.search(x) is not None)\n",
    "\n",
    "reduced = {k:v for k,v in token_counts.items() if has_letters(k)}\n",
    "print(\"Reduction due to all number matches: \", len(token_counts) - len(reduced))\n",
    "token_counts = reduced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "reduced = {k:v for k,v in token_counts.items() if len(k) > 1}\n",
    "print(\"Reduction due to single characters: \", len(token_counts) - len(reduced))\n",
    "token_counts = reduced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "token_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()\n",
    "input_wset_stems = {k: stemmer.stem(k) for k in token_counts.keys()}\n",
    "input_wset_stems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Catalog contains all the catagory score tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "catalog = []\n",
    "for i in range(len(DF)):\n",
    "    catalog.append({u[1].Stem: u[1].Score for u in DF[i].iterrows()})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rate the example text according to the scores:\n",
    "\n",
    "The scoring just adds up the word scores but this must be improved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Scores = []\n",
    "for cat_idx in range(len(fn)):\n",
    "    print(fn[cat_idx])\n",
    "\n",
    "    sum_score = 0\n",
    "    notin_catalog = 0\n",
    "    for p in token_counts.keys():\n",
    "        if p in input_wset_stems.keys():\n",
    "            r = input_wset_stems[p]\n",
    "        else:\n",
    "            r = p\n",
    "        \n",
    "        #print('.....')\n",
    "        if p in catalog[cat_idx].keys():\n",
    "            #sum_score += catalog[cat_idx][p] \n",
    "            #print(catalog[cat_idx][p], p,token_counts[p])\n",
    "            None\n",
    "        else:\n",
    "            #print('No Score', p,token_counts[p])\n",
    "            #notin_catalog+=1\n",
    "            None\n",
    "            \n",
    "        if r in catalog[cat_idx].keys():\n",
    "            sum_score += catalog[cat_idx][r] \n",
    "            print(catalog[cat_idx][r], r)\n",
    "        else:\n",
    "            print('No Score', r)\n",
    "            notin_catalog+=1\n",
    "\n",
    "    print('Count of Terms not found in the catalog:', notin_catalog)   \n",
    "    print('Score = ', sum_score)\n",
    "    print(CATEGORIES[cat_idx])\n",
    "    print('*'*30)\n",
    "    \n",
    "    Scores.append(sum_score)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize the score of the txt for each category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(len(CATEGORIES)):\n",
    "    print(CATEGORIES[i], Scores[i])\n",
    "print('-')\n",
    "for i in categories_desired_order:\n",
    "    print(CATEGORIES[i], Scores[i])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scoring in a nutshell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = ['https://www.un.org/sg/en/content/sg/speeches/2018-03-28/collective-action-improve-un-peacekeeping-operations-remarks',\n",
    "'https://www.un.org/sg/en/content/sg/speeches/2018-03-26/remembrance-victims-slavery-and-transatlantic-slave-trade-remarks',\n",
    "'http://www.let.rug.nl/usa/documents/1951-/martin-luther-kings-i-have-a-dream-speech-august-28-1963.php']\n",
    "\n",
    "import requests\n",
    "import json\n",
    "\n",
    "insightIP = 'http://178.62.229.16'\n",
    "insightPort = '8484'\n",
    "insightVersion = 'v1.0'\n",
    "apikey = 'thisisinsightapikey'\n",
    "\n",
    "# parameters of news recommendation are twitter_ids, crm_ids\n",
    "\"\"\"\n",
    "twitter_ids = []\n",
    "crm_ids = [135574293]\n",
    "insightSetting = insightIP + ':' + insightPort + '/api/' + insightVersion \n",
    "request = '/recommendation/news?' + 'twitter_ids=' + str(twitter_ids) + '&' + 'crm_ids=' + str(crm_ids)\n",
    "\"\"\"\n",
    "\n",
    "TXT = []\n",
    "\n",
    "for url in urls:\n",
    "    insightSetting = insightIP + ':' + insightPort + '/api/' + insightVersion\n",
    "    request = '/text_analytics/url_scraper?api_key='+apikey+'&url='+url\n",
    "    #res = requests.get(insightSetting + request)\n",
    "    print(insightSetting + request)\n",
    "    res = requests.get(insightSetting + request)\n",
    "    data = res.json()\n",
    "    TXT.append(data['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy as sc\n",
    "import pandas as pd\n",
    "\n",
    "import os\n",
    "\n",
    "import nltk\n",
    "from utils import tokenizer\n",
    "import nltk\n",
    "from nltk import FreqDist\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from numpy import log, mean\n",
    "import json, csv, re\n",
    "import pprint as pp\n",
    "\n",
    "\n",
    "path = '/Users/cemgil/src/OpenMaker/Semantics/output/'\n",
    "fn = os.listdir(path)\n",
    "\n",
    "DF = []\n",
    "CATEGORIES = []\n",
    "for f in fn:\n",
    "    print(f)\n",
    "    df = pd.read_csv(path+f)\n",
    "    DF.append(df)\n",
    "    #CATEGORIES.append(f.split('_')[1].split('.')[0])\n",
    "    CATEGORIES.append(f[9:].split('.')[0])\n",
    "\n",
    "    \n",
    "catalog = []\n",
    "for i in range(len(DF)):\n",
    "    catalog.append({u[1].Stem: u[1].Score for u in DF[i].iterrows()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Text2Score(txt, catalog):\n",
    "\n",
    "    with open(\"data/stopwords_standard.txt\", \"r\") as f:\n",
    "        STOP_WORDS_STANDARD = set(f.read().strip().split(\"\\n\"))\n",
    "    #print(STOP_WORDS_STANDARD)\n",
    "\n",
    "    with open(\"data/stopwords_openmaker.txt\", \"r\") as f:\n",
    "        STOP_WORDS_OPENMAKER = set(f.read().strip().split(\"\\n\"))\n",
    "    #print(STOP_WORDS_OPENMAKER)\n",
    "\n",
    "    # merging the two list together\n",
    "    STOP_WORDS = STOP_WORDS_STANDARD.union(STOP_WORDS_OPENMAKER)\n",
    "\n",
    "    tokens = nltk.word_tokenize(txt)\n",
    "    token_counts = FreqDist(tokens)\n",
    "    tokenizer.CHARACTERS_TO_SPLIT += '‘'+'’'+'“'+'”'+'.'\n",
    "\n",
    "    for stopword in STOP_WORDS:\n",
    "        if stopword in token_counts:\n",
    "            del token_counts[stopword]\n",
    "\n",
    "    for punctuation in tokenizer.CHARACTERS_TO_SPLIT:\n",
    "        if punctuation in token_counts:\n",
    "            del token_counts[punctuation]\n",
    "\n",
    "\n",
    "    pattern_letters = re.compile('[a-z]')\n",
    "    def has_letters(x):\n",
    "        return(pattern_letters.search(x) is not None)\n",
    "\n",
    "    reduced = {k:v for k,v in token_counts.items() if has_letters(k)}\n",
    "    #print(\"Reduction due to all number matches: \", len(token_counts) - len(reduced))\n",
    "    token_counts = reduced\n",
    "\n",
    "    reduced = {k:v for k,v in token_counts.items() if len(k) > 1}\n",
    "    #print(\"Reduction due to single characters: \", len(token_counts) - len(reduced))\n",
    "    token_counts = reduced\n",
    "\n",
    "    stemmer = PorterStemmer()\n",
    "    input_wset_stems = {k: stemmer.stem(k) for k in token_counts.keys()}\n",
    "\n",
    "    Scores = []\n",
    "    for cat_idx in range(len(fn)):\n",
    "        sum_score = 0\n",
    "        notin_catalog = 0\n",
    "        for p in token_counts.keys():\n",
    "            if p in input_wset_stems.keys():\n",
    "                r = input_wset_stems[p]\n",
    "            else:\n",
    "                r = p\n",
    "\n",
    "            if p in catalog[cat_idx].keys():\n",
    "                #sum_score += catalog[cat_idx][p] \n",
    "                #print(catalog[cat_idx][p], p,token_counts[p])\n",
    "                None\n",
    "            else:\n",
    "                #print('No Score', p,token_counts[p])\n",
    "                #notin_catalog+=1\n",
    "                None\n",
    "\n",
    "            if r in catalog[cat_idx].keys():\n",
    "                sum_score += catalog[cat_idx][r] \n",
    "            else:\n",
    "                notin_catalog+=1\n",
    "\n",
    "        Scores.append(sum_score)\n",
    "\n",
    "    return Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for txt in TXT:\n",
    "    Score = Text2Score(txt, catalog)\n",
    "    categories_desired_order = [4,0,3,7,6,9,1,8,2,5]\n",
    "\n",
    "    CatScore = [(CATEGORIES[i], Score[i]-min(Score)) for i in categories_desired_order]    \n",
    "    print(CatScore)\n",
    "    RadarPlot(CatScore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Plots a radar chart.\n",
    "\n",
    "from math import pi\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def RadarPlot(CatScore):\n",
    "    # Set data\n",
    "    cat = [a[0] for a in CatScore]\n",
    "    values = [np.abs(a[1]) for a in CatScore]\n",
    "\n",
    "    N = len(cat)\n",
    "\n",
    "    x_as = [n / float(N) * 2 * pi for n in range(N)]\n",
    "\n",
    "    # Because our chart will be circular we need to append a copy of the first \n",
    "    # value of each list at the end of each list with data\n",
    "    values += values[:1]\n",
    "    x_as += x_as[:1]\n",
    "\n",
    "\n",
    "    # Set color of axes\n",
    "    plt.rc('axes', linewidth=0.5, edgecolor=\"#888888\")\n",
    "\n",
    "\n",
    "    # Create polar plot\n",
    "    ax = plt.subplot(111, polar=True)\n",
    "\n",
    "\n",
    "    # Set clockwise rotation. That is:\n",
    "    ax.set_theta_offset(pi / 2)\n",
    "    ax.set_theta_direction(-1)\n",
    "\n",
    "\n",
    "    # Set position of y-labels\n",
    "    ax.set_rlabel_position(0)\n",
    "\n",
    "\n",
    "    # Set color and linestyle of grid\n",
    "    ax.xaxis.grid(True, color=\"#888888\", linestyle='solid', linewidth=0.5)\n",
    "    ax.yaxis.grid(True, color=\"#888888\", linestyle='solid', linewidth=0.5)\n",
    "\n",
    "\n",
    "    # Set number of radial axes and remove labels\n",
    "    plt.xticks(x_as[:-1], [])\n",
    "\n",
    "    # Set yticks\n",
    "    plt.yticks([],[])\n",
    "\n",
    "\n",
    "    # Plot data\n",
    "    ax.plot(x_as, values, linewidth=0, linestyle='solid', zorder=3)\n",
    "\n",
    "    # Fill area\n",
    "    ax.fill(x_as, values, 'b', alpha=0.3)\n",
    "\n",
    "\n",
    "    # Set axes limits\n",
    "    plt.ylim(0, max(values))\n",
    "\n",
    "\n",
    "    # Draw ytick labels to make sure they fit properly\n",
    "    for i in range(N):\n",
    "        angle_rad = i / float(N) * 2 * pi\n",
    "\n",
    "        if angle_rad == 0:\n",
    "            ha, distance_ax = \"center\", 10\n",
    "        elif 0 < angle_rad < pi:\n",
    "            ha, distance_ax = \"left\", 1\n",
    "        elif angle_rad == pi:\n",
    "            ha, distance_ax = \"center\", 1\n",
    "        else:\n",
    "            ha, distance_ax = \"right\", 1\n",
    "\n",
    "        ax.text(angle_rad, max(values) + distance_ax, cat[i], size=10, horizontalalignment=ha, verticalalignment=\"center\")\n",
    "\n",
    "\n",
    "    # Show polar plot\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
