{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discovery and Representation of Open Making Related Terms\n",
    "\n",
    "Bulent Ozel, UZH\n",
    "\n",
    "```bulent.ozel@gmail.com```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It should noted that the primary focus of the text processing in this process is to be able to collect, clean, tokenize and prune an input text around **one or very few specific theme(s) or topic(s)**.\n",
    "\n",
    "In cases when there are multiple themes with statsitically large and sufficient set of documents then standard topic modelling techniques such as NMF, LDA, LSI are suggested for the task. Those standarized approaches are based on matrix decomposition techniques and can be employed to capture and measure the specificity of terms around the topics.\n",
    "\n",
    "Nevertheless, the normalized term frequencies and as well as the specificity scores associated to them with respect to a reference background corpus, can be used as input to other matrix decomposition techniques when multiple themes grouped together terms' specificity at being able to diffenntiate these themes from each other is the concern.\n",
    "\n",
    "\n",
    "## 1. Objective\n",
    "\n",
    "This notebook sketches the initial exercise on identifiying a weighted set of key terms using a specific corpus.\n",
    "\n",
    "The method outlined here aims to set-up a base line for future improvements. See the relevant sections below.\n",
    " * It uses a statistical approach combined with standardized procedures that are widely applied in standard NLP workflows.\n",
    " * In this base line, it aims to present a work flow that can be ablied to\n",
    "     * different languages\n",
    "     * differrent problem domains\n",
    " * It relies on a domain specific corpus as a foreground corpus and a reference corpus as the background corpus\n",
    " * In this work flow specific corpus is formed via supervised crawling on the wikipedia\n",
    " * In the exercises below the NLTK's Brown corpus is used as the reference background corpus. However, in the next work round, we aim to use full Wikipedia corpus under the same language of the specific corpus or a reprsentative random sample of it. \n",
    " * The normalized comparison of candidate keywords within specific vs reference corpus is used as a proxy indication of the relevance of the candidate keyword for the given topic that is reprseneted by the collection of articles under the specific corpus.\n",
    "\n",
    "## 2. Overall Work Flow\n",
    "In short, the workflow presented on this notebook is the second stage on a workflow objective of which is being able to measure relevance of a given external input to a specific theme, issue or topic. The steps of the work flow is as follows.\n",
    "\n",
    "1. Forming a specific corpus where the corpus consists of set of documents around a topic. The corpus could be\n",
    "    - a set of blog articles around an issue let say green finance\n",
    "    - or a set of Wikipedia articles around the same subject\n",
    "    - or collection of news articles around the green finance\n",
    "    - or collection of tweets around the same issue.\n",
    "    \n",
    "    At the moment we have another module that given a set of seed Wikipedia articles around an issue the crawler scrapes textual data from articles. For the details of the module please [see the scraper module.](https://github.com/bulentozel/OpenMaker/tree/master/Scraping). The output of that module is a set of input texts stored in a collection in JSON format. \n",
    "\n",
    "2. Given an input set of texts on a theme a concept or a topic identify set of terms that more likely or less likely can occur within a discussion on the topic. This module hereby presents one of the simple methods for this purpose.\n",
    "\n",
    "3. Given a list of weighted terms which are more likely to occur or reprsent a theme, concept or topic and input query text measure the relevance of the input text to the topic/theme/concept. [The notebook in this link](https://github.com/bulentozel/OpenMaker/blob/master/Semantics/Score%20Text.ipynb) demonstrates one way doing such scoring of a given text against the curated set of terms of this particular module.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## 3. Work-flow Process in this Particular Module\n",
    "1. Loading the reference corpus and a topic specific corpus\n",
    "2. Tokenizing and cleaning the reference corpus and the specific corpus\n",
    "3. Calculating term frequency counts of the reference corpus and the specific corpus\n",
    "4. Identifying common terms in both corpuses\n",
    "5. Identifying distinctive terms that occur in the specific corpus but not in reference corpus, if any\n",
    "6. Reducing the terms in the dinstinctive set by an iterative manual inspection process as well as by  using a curated list of distinctive terms on the topic.\n",
    "7. Computing likelihood ratio (empirical probabilities) of the terms that are observed in the specific topic\n",
    "8. Tabulating identified list of terms, their raw frequencies and weights.\n",
    "\n",
    "## 4. Suggested Future Work\n",
    "\n",
    "* Using language specicif term frequency counts of Wikipedia itself for comparisons. In NLP terminology, the *foreground* corpus around a topic needs to be compared and contrasted to a *background* corpus.\n",
    "\n",
    "* Improving the semantic crawler of the previous stage to be able to increase quality of the specific corpuses\n",
    "\n",
    "* Adding new scoring types that measures relevance of a given term to a given topic.\n",
    "\n",
    "### Methodological Improvements\n",
    "* Instead of tokenizing all terms, examine possibilities of key-phrase extrcation combining with *tf-idf* and \n",
    "    - experiment with extracting noun phrases and words, for this use NLTK's regular expression module for POS (part of speeach) analysis.\n",
    "    - extract n-grams where n=1,2,3\n",
    "\n",
    "## 5. Definitions and Assumptions\n",
    "\n",
    "### Assumptions\n",
    "* In the current state of the task it is assumed that a document's terms tend to be relatively frequent within the document as compared to an external reference corpus. However, it should be noted this assumption is contested in the field. See the paper by Chuang et el.\n",
    "\n",
    "* Condidering the fact that the crawler is used to aggregate semantically related set of documents into a single document, *tf x idf* is equivalent to *tf*. As can be seen below, we use a normalized version of *tf*: *ntS / NS*.\n",
    "\n",
    "* Fewer number of but relatively more relevant training (input corpus) is prefered in order to reduce term extraction problems due to length of documents. However, it should be noted that the crawling depth of an identiefied wiki article from stage 1 of this document can be used as an additional weight on relevance/reprsesntation of keywords.\n",
    "\n",
    "* We have limited ourselves to terms instead of n-grams and phrases or use of POS to be able to develop a base model that can work on different languages.\n",
    " \n",
    " \n",
    "### Term\n",
    "Given for instance a set of texts around open source software movement a term that is identified can be a word such as *openness*, a person such as *Stallman* a license type such as *GNU*, an acronym for an organization such as *FSF* the Free Software Foundation, or a technology such as *Emacs*.\n",
    "\n",
    "### Likelihood ratio\n",
    "It is a simple measure computed comparing frequency count of a term in a specific corpus versus its frequency count in the reference reference corpus. Here assumption is that the reference corpus is a large enough sample of the language at observing the occurance of a term. Then having a higher/lower observation frequency of a term in the specific corpus is a proxy indicator for the term choice while having a debate on the topic.\n",
    "\n",
    "The likelihood ratio for a term P_t is calculated as:\n",
    "\n",
    "P_t = log ( (ntS/NS) / (ntR/NR) )\n",
    "\n",
    "where\n",
    "\n",
    "- *ntS* is the raw frequency count of the term in the entire specific corpus\n",
    "- *ntR* is the raw frequenccy count of the term in the reference corpus\n",
    "- *NS* is the total number of terms in the specific corpus\n",
    "- *NR* is the total number of terms in the reference corpus\n",
    "\n",
    "It should be noted that frequency counts are calculated after having applied the same tokenization and post processing such as excluding stop-words, pancuations, rare terms, etc both on the reference corpus and the specific corpus.\n",
    "\n",
    "## 6. State of the art \n",
    "\n",
    "* Survey Paper: Kazi Saidul Hasan and Vincent Ng, 2014. “Automatic Keyphrase Extraction: A Survey of the State of the Art” Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 1262–1273.\n",
    "\n",
    "* Survey Paper: Sifatullah Siddiqi and Aditi Sharan. Article: Keyword and Keyphrase Extraction Techniques: A Literature Review. International Journal of Computer Applications 109(2):18-23, January 2015\n",
    "\n",
    "* Survey Paper: Z. A. Merrouni, B. Frikh, and B. Ouhbi. Automatic keyphrase extraction: An overview of the state of the art. In 2016 4th IEEE\n",
    "Colloquium on Information Science and Technology\n",
    "(CiSt), pages 306–313, Oct 2016\n",
    "\n",
    "* PageRank - Topical: Zhiyuan Liu, Wenyi Huang, Yabin Zheng and Maosong Sun, 2010. “Automatic Keyphrase Extraction via Topic Decomposition”. Proceeding EMNLP '10 Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing Pages 366-376 \n",
    "\n",
    "* RAKE (Rapid Automatic Keyword Extraction ): Stuart Rose, Dave Engel, Nick Cramer, and Wendy Cowley. Automatic\n",
    "keyword extraction from individual documents. Text Mining, pages 1–20, 2010.\n",
    "\n",
    "* TextRank - Graph Based : Rada Mihalcea and Paul Tarau. Textrank: Bringing order into texts.\n",
    "Association for Computational Linguistics, 2004.\n",
    "\n",
    "* STOPWORDS: S. Popova, L. Kovriguina, D. Mouromtsev, and I. Khodyrev. Stopwords\n",
    "in keyphrase extraction problem. In 14th Conference\n",
    "\n",
    "* Corpus Similarity - Keyword frequency based: Adam Kilgarriff. Using word frequency lists to measure corpus homogeneity and similarity between corpora. In Proceedings of ACLSIGDAT Workshop on very large corpora, pages 231–245, 1997.\n",
    "\n",
    "* Recommendation - Keyphrase Based: F. Ferrara, N. Pudota and C. Tasso. A keyphrase-based paper recommender system. In: Digital Libraries and Archives. Springer Berlin Heidelberg, 2011. p. 14-25.\n",
    "\n",
    "* Jason Chuang, Christopher D. Manning, Jeffrey Heer, 2012. \"Without the Clutter of Unimportant Words\": Descriptive Keyphrases for Text Visualization\" ACM Trans. on Computer-Human Interaction, 19(3), 1–29.\n",
    "\n",
    "## Appendix\n",
    "\n",
    "### Scoring\n",
    "\n",
    "    \n",
    "Using the outcome of this technique to score arbitrary input texts against a single issue such as financial sustainability or against a set of issues such as the 10 basic human values requires a set of normalization of the raw scores and their rescaling/transformation.\n",
    "\n",
    "The factors that need to be considered are:\n",
    "\n",
    "- **Differing document lengths:** The likelihood of repetion of a key phrase increases as the size of the input text gets larger. In more concrete terms, when a scoring that simply sums up detection of weighted keyphrases or words within a given input text would be very sensitive to the document length. For isntance, the an executive summary of an article would very likely get quite lower score than the full article on any issue.\n",
    "\n",
    "    *Among other methods, this can simply be resolved by computing per word scores, where the word set to be conidered is the tokenized and cleaned set of words that represent the input text.*\n",
    "\n",
    "\n",
    "- **Topical relevance:** This factor would be important when the subject matter of the inputs texts vary among each other. In other words, this factor would matter to a very high significance, let's say when one wants to compare perceptions of indivuduals on the role of traditions in the personal lives and when this question is not asked them in a uniform manner that under the same social, cultural, environmental and physical conditions.\n",
    "    \n",
    "    Let’s assume that issue under investigation is again the perception and role of traditions in personal lives. It is possible that the same blogger with a strong opinion on traditions (i) may not touch the issue while talking on data science, (ii) he would slightly touch the issue while he talks about her preferences in mobile devices (iii) He dives into subject using all keywords and phrases when he talks about impact of traditions on social order. In brief, it is necessary to offset the variability of the topical relavance of an input text to the issue under investigation when arbitrary text samples are used for scoring.\n",
    "   \n",
    "    *An offsetting scheme can be devised when opinion or perception of an actor is to be measured with respect to more than one factor that define the issue under investigation. For instance, when we want to measure the position of a political leader on individual liberties vs social security or when we want to profile discourse of the political leader as of a ten basic human values we could employ some simple statistical methods in order to offset the topical relevance of the discourses or the speeches of the political figure to what we would like to measure.*\n",
    "    \n",
    "    *A simple method could be rescaling the scores on each sub factor such as the scores of liberty and security that are measured from the same speech into a range of -1 to 1. This can simply be done by taking the mean of the two and then deducting the mean from each score and scaling them into a scala of -1 to 1. This way it may be possible to use multiple speeches of the same political figure on different topics to evalaute his or her postion on liberty vs security matter.*\n",
    "   \n",
    "   In statistical terms this problem corresponds to adjusting or normalizing ratings or scores measured on different scales to a notionally common scale. Given the fact that in most cases a normal distribution for underlying factors may not be assumed the quantile-normalization technique is suggested. The quantile normalization sorts and ranks the variables with a non-negative amplitudes. Then these rankings can be scaled to for instance to a 0-1 interval. \n",
    "\n",
    "\n",
    "- **Level of subjectivity**.  This is variability in terms of relevant importance attributed to each issue given a set of issues. For instance, according to a study (Schwartz 2012), it is seen that almost all cultures around the world attach a higher importance to ‘universalism’ human value than the ’power’ basic human value. So when the objective of the scoring is not simply to rank of importance attached to each of them, then a comparative importance with respect to overall observations needs to be tackled.\n",
    "\n",
    "    *Observed variances in each query texts can be considered. That is, a simple statistical methods can be used for instance to be able to compare two or more query texts with respect to each other. A suggested method would be (1) estimate coefficient of variation for each input text using per-word scores (2) the rescale quantile-normalized scores that is suggested above using the estimated coefficient of variation in each case.*\n",
    "    \n",
    "    *When this rescaling is applied for instance to universalism versus power or liberty vs security the coeffcient of variation would act as a polarization measure.*\n",
    "\n",
    "#### Scoring and value system profiling\n",
    "\n",
    "When one attempts to use scores, for instance, around the basic ten human values and one wants to construct the value system of the person, then both ranking of the scores as well as the relevant importance of each score from a number of texts from the same person should be taken into consideration.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## External python modules\n",
    "The notebook below should work with both Pyhton 2 and Python 3 kernels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import FreqDist\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from numpy import log, mean\n",
    "import json, csv, re, copy\n",
    "import pprint as pp\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implemented: packages, modules, objects and functions\n",
    "*Still under construction!*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from utils import tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Corpus(object):\n",
    "    \"\"\"A generic class to be used for foreground or background corpuses.\n",
    "\n",
    "    Attributes:\n",
    "        tf_dist: (:obj:`nltk.FreqDist`): An NLTK container for tokenized and cleaned\n",
    "            terms in the corpus.\n",
    "        stemmer (x :obj:`str` -> y: :obj:`str`): A stemmer function.\n",
    "        stems (:obj:`dict`): A dictionary of terms and their stems\n",
    "        labels (:obj:`dict`): Term level labels.\n",
    "        scores (:obj:`dict`): A dictionary of terms and their corpus sepcificity scores\n",
    "            as of a reference corpus\n",
    "        ref (:obj:`dict`): A dictionary of terms that holds normalized occurance frequency of\n",
    "            the term at a reference corpus.\n",
    "        sepficifs (:obj:`set`): A set of terms appointed/associated with the corpus externally.        \n",
    "        To be implemented:\n",
    "            texts_raw (:obj:`json`): A JSON collection of raw texts of the corpus.\n",
    "        \n",
    "            texts_clean (:obj:`json`): A JSON collection of cleaned/processed\n",
    "            texts of the corpus.\n",
    "        \n",
    "            tf_idf (:obj:`json`): Tf-Idf analyses of the corpus (to be implemented).\n",
    "        \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, tf_dist, stemmer = None):\n",
    "        \"\"\"The class constructor.\n",
    "\n",
    "        Args:\n",
    "            tf_dist (:obj:`nltk.FreqDist`): An NLTK container for tokenized and cleaned\n",
    "            terms in the corpus.\n",
    "            stemmer (x :obj:`str` -> y: :obj:`str`): A stemmer function (default None)\n",
    "\n",
    "        \"\"\"\n",
    "        self.tf_dist = tf_dist\n",
    "        self.stemmer = stemmer if stemmer else lambda x:x\n",
    "        self.stems = dict()\n",
    "        self.labels = dict()\n",
    "        self.scores = dict()\n",
    "        self.ref = dict()\n",
    "        self.specifics = set()\n",
    "        \n",
    "\n",
    "    def get_top_frequents(self, top=42):\n",
    "        \"\"\"The method identifies and returns top frequent terms.\n",
    "        \n",
    "        Args:\n",
    "            top (int): Size of the short list (default 42).\n",
    "            \n",
    "        Returns:\n",
    "            (:obj:`list` of :obj:`tuple` of :obj:`str` and :obj:`int`): Returns the frequency dist\n",
    "                for top terms as list of tuples of term and frequency pairs.\n",
    "\n",
    "        \"\"\"\n",
    "        return self.tf_dist.most_common(top)\n",
    "    \n",
    "    def get_least_frequents(self, bottom=42):\n",
    "        \"\"\"The method identifies and returns least frequent terms.\n",
    "        \n",
    "        Args:\n",
    "            bottom (int): Size of the short list (default 42).\n",
    "            \n",
    "        Returns:\n",
    "            (:obj:`list` of :obj:`tuple` of :obj:`str` and :obj:`int`): Returns the frequency dist\n",
    "                for the least frequent terms as list of tuples of term and frequency pairs.\n",
    "\n",
    "        \"\"\"\n",
    "        _n = self.get_count_uniques()\n",
    "        slice_from = _n - bottom if _n > bottom else 0\n",
    "        return self.tf_dist.most_common(_n)[slice_from:]\n",
    "    \n",
    "    def get_count_uniques(self):\n",
    "        \"\"\"The method identifies and returns top frequent terms.\n",
    "            \n",
    "        Returns:\n",
    "            (:obj:`int`): Returns an integer.\n",
    "\n",
    "        \"\"\"\n",
    "        return self.tf_dist.B()\n",
    "\n",
    "    def get_size(self):\n",
    "        \"\"\"The returns the size of the corpus in terms of number of terms it has.\n",
    "            \n",
    "        Returns:\n",
    "            (:obj:`int`): Returns an integer. It is summation of raw frequency counts.\n",
    "\n",
    "        \"\"\"\n",
    "        return self.tf_dist.N()\n",
    "    \n",
    "    def union(self, other,\n",
    "              as_corpus = False,\n",
    "              stats = False):\n",
    "        \"\"\"The method identifies and returns the union of two corpora.\n",
    "        \n",
    "        Args:\n",
    "            other (:obj:`Corpus`): An instance of this Class object.\n",
    "            as_corpus (bool): When True it returns a new Corpus (default False).\n",
    "            stats (bool): When True and as_corpus is false returns the frequency \n",
    "                count of the union.\n",
    "            \n",
    "        Returns:\n",
    "            (:obj:`list` of :obj:`str`): If as_corpus is False and stats is False \n",
    "                it returns the list of union of terms in both cases.\n",
    "            (:obj:`list` of :obj:`tuple` of :obj:`str` and :obj:`int`): Returns the frequency dist\n",
    "                for the union terms, ff as_corpus is False and stats is True. \n",
    "            (:obj:`Corpus`): In all other cases it returns a nrew Corpus class for the intersection.\n",
    "                Frequencies are the minimum of the two occurances.\n",
    "        \n",
    "        Examples:\n",
    "            >>> Corpus(FreqDist('abbbc')).union(Corpus(FreqDist('bccd')), stats = True)\n",
    "            [('a', 1), ('b', 3), ('c', 2), ('d', 1)]\n",
    "\n",
    "        \"\"\"\n",
    "        Union = self.tf_dist | other.tf_dist\n",
    "        if not as_corpus and not stats: return list(Union.keys())\n",
    "        if not as_corpus and stats: return list(Union.items())\n",
    "        return Corpus(Union)\n",
    "    \n",
    "    def intersection(self, other,\n",
    "                     as_corpus = False,\n",
    "                     stats = False):\n",
    "        \"\"\"The method identifies and returns the intersection of two corpora.\n",
    "        \n",
    "        Args:\n",
    "            other (:obj:`Corpus`): An instance of this Class object.\n",
    "            as_corpus (bool): When True it returns a new Corpus (default False).\n",
    "            stats (bool): When True and as_corpus is false returns the frequency \n",
    "                count of the intersections.\n",
    "            \n",
    "        Returns:\n",
    "            (:obj:`list` of :obj:`str`): If as_corpus is False and stats is False \n",
    "                it returns the list of joint terms.\n",
    "            (:obj:`list` of :obj:`tuple` of :obj:`str` and :obj:`int`): Returns the frequency dist\n",
    "                for the joint terms, if as_corpus is False and stats is True. \n",
    "            (:obj:`Corpus`): In all other cases it returns a nrew Corpus class for the intersection.\n",
    "                Frequencies are the minimum of the two occurances.\n",
    "\n",
    "        \"\"\"\n",
    "        Common = self.tf_dist & other.tf_dist\n",
    "        if not as_corpus and not stats: return list(Common.keys())\n",
    "        if not as_corpus and stats: return list(Common.items())\n",
    "        return Corpus(Common)\n",
    "    \n",
    "    def difference(self, other,\n",
    "              as_corpus = False,\n",
    "              stats = False):\n",
    "        \"\"\"The method identifies and returns the difference of the self from the other.\n",
    "        \n",
    "        Note:\n",
    "            Implementation needs style and refactoring.\n",
    "            \n",
    "        Args:\n",
    "            other (:obj:`Corpus`): An instance of this Corpus Class object.\n",
    "            as_corpus (bool): When True it returns a new Corpus (default False).\n",
    "            stats (bool): When True and as_corpus is false returns the frequency \n",
    "                count of the difference set.\n",
    "            \n",
    "        Returns:\n",
    "            (:obj:`dict`): A dictionary of terms and their frequency counts.\n",
    "            \n",
    "        \"\"\"\n",
    "        joint = set(self.intersection(other))\n",
    "        difference = set(self.tf_dist.keys()).difference(joint)\n",
    "        return {k:v for k, v in self.tf_dist.items() if k in difference}\n",
    "    \n",
    "    def plot(self, top, cumulative = False):\n",
    "        \"\"\"Plotting.\n",
    "        \n",
    "        Note:\n",
    "            \n",
    "        Returns:\n",
    "            (:obj:`bool`): True\n",
    "            \n",
    "        \"\"\"\n",
    "        self.tf_dist.plot(top, cumulative = cumulative)\n",
    "        return True\n",
    "    \n",
    "    def tabulate(self, top):\n",
    "        \"\"\"Tabulating.\n",
    "        \n",
    "        Note:\n",
    "            Works better when used to see few top terms.\n",
    "            \n",
    "        Returns:\n",
    "            (:obj:`bool`): True\n",
    "            \n",
    "        \"\"\"\n",
    "        self.tf_dist.tabulate(top) \n",
    "        return True\n",
    "    \n",
    "    def list_terms(self):\n",
    "        \"\"\"It returns the list terms in the corpus\n",
    "        \n",
    "        Note:\n",
    "            Implementation needs refactoring.\n",
    "            \n",
    "        Returns:\n",
    "            (:obj:`list`): An alphabetically sorted list.\n",
    "            \n",
    "        \"\"\"\n",
    "        return list(sorted(self.tf_dist.keys()))\n",
    "\n",
    "    \n",
    "    def set_stemmer(self, stemmer):\n",
    "        \"\"\"The appointing a new stemmer function to the corpus.\n",
    "        \n",
    "        Args:\n",
    "            stemmer (x :obj:`str` -> y: :obj:`str`): A stemmer function (default None)\n",
    "            \n",
    "        Returns:\n",
    "            (:obj:`bool`): True\n",
    "            \n",
    "        \"\"\"\n",
    "        \n",
    "        self.stemmer = stemmer\n",
    "        return True\n",
    "        \n",
    "    \n",
    "    def compute_stems(self):\n",
    "        \"\"\"The function returns the a dictionary of terms and their corresponding stems.\n",
    "        \n",
    "        Args:\n",
    "            stemmer (x :obj:`str` -> y: :obj:`str`): A stemmer function (default None)\n",
    "            \n",
    "        Returns:\n",
    "            (:obj:`bool`): True\n",
    "            \n",
    "        \"\"\"\n",
    "        self.stems = {k: self.stemmer(k) for k in self.list_terms()}\n",
    "        return True\n",
    "    \n",
    "    \n",
    "    def get_stems(self):\n",
    "        \"\"\"The function returns the a dictionary of terms and their corresponding stems.\n",
    "        \n",
    "        Args:\n",
    "            \n",
    "        Returns:\n",
    "            (:obj:`dict`): A dictionary of {term:stem of the term}.\n",
    "            \n",
    "        \"\"\"\n",
    "        if self.stems: return self.stems\n",
    "        self.compute_stems()\n",
    "        return self.stems\n",
    "    \n",
    "    def label(self, marker, labels=None):\n",
    "        \"\"\"The function labels yet not labeled terms according to the user defined scheme.\n",
    "        \n",
    "        Args:\n",
    "            marker (x :obj:`str` -> y: :obj:`str`): A marker function\n",
    "            \n",
    "        Returns:\n",
    "            (:obj:`dict`): A dictionary of {term:label of term}.\n",
    "            \n",
    "        \"\"\"\n",
    "        if not labels: labels = {t:None for t in self.list_terms()}\n",
    "        for t,v in labels.items():\n",
    "            if v: continue\n",
    "            labels[t] = marker(t)\n",
    "        self.labels = labels\n",
    "        return self.labels\n",
    "    \n",
    "    def set_specific_set(self, terms):\n",
    "        \"\"\"The function sets the set of corpus specific terms.\n",
    "        \n",
    "        Args:\n",
    "            terms (:obj:`set`): The list of corpus specific terms\n",
    "            \n",
    "        Returns:\n",
    "            (:obj:`bool`): True.\n",
    "            \n",
    "        \"\"\"\n",
    "        self.specifics = set(terms)\n",
    "        return True\n",
    "    \n",
    "    def to_pandas(self):\n",
    "        \"\"\"The function exports its data into pandas dataframe.\n",
    "        \n",
    "        Note:\n",
    "            ToDo: The function needs parameterization, generalization and error checks.\n",
    "        \n",
    "        Returns:\n",
    "            (:obj:`pandas.DataFrame`) The tabulated data\n",
    "            \n",
    "        \"\"\"\n",
    "        wTFd = {w:self.tf_dist.freq(w) for w,c in self.tf_dist.items()}\n",
    "        wTF = pd.Series(wTFd, name='wTF')\n",
    "        wTF.index.name = 'Term'\n",
    "        TF = pd.Series(dict(self.tf_dist.items()), name='TF')\n",
    "        TF.index.name = 'Term'\n",
    "        Stem = pd.Series(self.stems, name='Stem')\n",
    "        TF.index.name = 'Term'\n",
    "        Score = pd.Series(self.scores, name='Score')\n",
    "        Score.index.name = 'Term'\n",
    "        SType = pd.Series(self.labels, name='sType')\n",
    "        SType.index.name = 'Term'\n",
    "        wTFref = pd.Series(self.ref, name='wTFref')\n",
    "        wTFref.index.name = 'Term'\n",
    "        DF = Stem.to_frame()\n",
    "        DF = DF.merge(Score.to_frame(), left_index=True, right_index=True)\n",
    "        DF = DF.merge(SType.to_frame(), left_index=True, right_index=True)\n",
    "        DF = DF.merge(TF.to_frame(), left_index=True, right_index=True)\n",
    "        DF = DF.merge(wTF.to_frame(), left_index=True, right_index=True)\n",
    "        DF = DF.merge(wTFref.to_frame(), left_index=True, right_index=True)\n",
    "        return DF\n",
    "               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def format_output_fname(current_theme):\n",
    "    output_fname = \"_\".join([word.capitalize() for word in current_theme.split(\" \")])\n",
    "    return output_fname\n",
    "\n",
    "\n",
    "def load_from_file(fname):\n",
    "    \"\"\"The method reloads a new stopwords list.\n",
    "\n",
    "    Note:\n",
    "        Internal stopword is overwritten.\n",
    "\n",
    "    Args:\n",
    "        fname (:obj:`str`): a file path string\n",
    "\n",
    "    Returns:\n",
    "        (:obj:`set`): The list of terms\n",
    "\n",
    "    Raises:\n",
    "        FileNotFoundError: Raised if a given file is not accessable.\n",
    "\n",
    "    \"\"\"\n",
    "    with open(fname, \"r\") as f:\n",
    "        return list(f.read().strip().split(\"\\n\"))\n",
    "\n",
    "def filter_pandas_rows(df, col = 'Score', min_t= None, max_t = None):\n",
    "    \"\"\"The method extracts rows from a Pandas data frame for the given score range.\n",
    "    The scores above the minimum and below the maximum is selected.\n",
    "\n",
    "    Note:\n",
    "        This function should be generalized so that it can work on any predicate function.\n",
    "\n",
    "    Args:\n",
    "        df (:obj:`pandas.core.frame.DataFrame`): A Pandas data frame.\n",
    "        col (:obj:`str`): The column that the filtering operation to be applied (default 'Score') \n",
    "        min_t (:obj:`float`): The minumum score threshold to be included when assigned (default None).\n",
    "        max_t (:obj:`float`): The maximum score threshold to be included when assigned (default None).\n",
    "\n",
    "    Returns:\n",
    "        df (:obj:`pandas.core.frame.DataFrame`): A Pandas data frame.\n",
    "    \n",
    "    Raises:\n",
    "        TypeError: Raised if the column data type is not a number.\n",
    "\n",
    "    \"\"\"\n",
    "    col_type = str(df.dtypes[col])\n",
    "    if not ('float' in col_type or 'int' in col_type):\n",
    "        raise TypeError('Column type should be either int or float')\n",
    "        \n",
    "    if min_t:\n",
    "        df = df.loc[df[col] >= min_t]\n",
    "    \n",
    "    if max_t:\n",
    "        df = df.loc[df[col] <= max_t]\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class TextCleaner(object):\n",
    "    \"\"\"An object that contains a set of tools to clean and preprocess textual data.\n",
    "\n",
    "    Note:\n",
    "        The object uses nltk.FreqDist object\n",
    "        For stem checks during pruninng in needs and external stemmer.\n",
    "    \n",
    "    Attributes:\n",
    "        exceptions (:obj:`list` of :obj:`str`): List of excepted terms.\n",
    "        stopwords (:obj:`list` of :obj:`str`): List of stopwords.\n",
    "        stemf: A stemmer funtion.\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self,\n",
    "                 stopwords = [],\n",
    "                 exceptions = [],\n",
    "                 fstemmer = lambda x:x, \n",
    "                 stemcheck = False):\n",
    "        \"\"\"The class constructor.\n",
    "\n",
    "        Args:\n",
    "            stopwords (:obj:`list` of :obj:`str`, optional): list of stopwords (default None).\n",
    "            exceptions (:obj:`set` of :obj:`str`, optional): list of excepted terms (default None).\n",
    "            fstemmer (x :obj:`str` -> y: :obj:`str`, optional): A stemmer function (default f(x) = x)\n",
    "            stemcheck (:obj:`bool`): A flag to determine whether stems of exceptions should also be considered\n",
    "                (default False)\n",
    "            \n",
    "        \"\"\"\n",
    "        try:\n",
    "            from nltk import FreqDist\n",
    "        except:\n",
    "            raise  ImportError(\"It wasn't possible to import 'nltk.FreqDist.\")\n",
    "            \n",
    "        \n",
    "        self.stopwords = stopwords\n",
    "        self.stemf =  fstemmer\n",
    "        self.exceptions = self.make_exceptions(exceptions, stemcheck)\n",
    "     \n",
    "    \n",
    "    def load_stopwords(self, spointer):\n",
    "        \"\"\"The method reloads a new stopwords list.\n",
    "\n",
    "        Note:\n",
    "            Internal stopword is overwritten.\n",
    "            \n",
    "        Args:\n",
    "            spointer (:obj:`list` of :obj:`str`or :obj:`str`): Either file path string or\n",
    "                a list of stopwords.\n",
    "\n",
    "        Returns:\n",
    "            bool: True if successful, False otherwise.\n",
    "\n",
    "        Raises:\n",
    "            FileNotFoundError: Raised if a given file is not accessable.\n",
    "\n",
    "        \"\"\"\n",
    "        if isinstance(spointer, (list, set)):\n",
    "            self.stopwords = set(spointer)\n",
    "            return True\n",
    "        \n",
    "        with open(spointer, \"r\") as f:\n",
    "            self.stopwords = set(f.read().strip().split(\"\\n\"))    \n",
    "        return True\n",
    "    \n",
    "    def extend_stopwords(self, spointer):\n",
    "        \"\"\"The method extends a new stopwords list.\n",
    "            \n",
    "        Args:\n",
    "            spointer (:obj:`list` of :obj:`str`, :obj:`str`): Either file path string or\n",
    "                a list of stopwords.\n",
    "\n",
    "        Returns:\n",
    "            bool: True if successful, False otherwise.\n",
    "\n",
    "        Raises:\n",
    "            FileNotFoundError: Raised if a given file is not accessable.\n",
    "\n",
    "        \"\"\"\n",
    "        if isinstance(spointer, (list, set)):\n",
    "            sws = set(spointer)\n",
    "        else:\n",
    "            with open(spointer, \"r\") as f:\n",
    "                newwords = set(f.read().strip().split(\"\\n\"))\n",
    "                sws = set(newwords)\n",
    "        if not sws: return False\n",
    "        self.stopwords = self.stopwords.union(sws) if self.stopwords else sws\n",
    "        return True\n",
    "    \n",
    "    @staticmethod\n",
    "    def freq_dist(words):\n",
    "        \"\"\"The static method computes frequency distribution of a word list.\n",
    "            \n",
    "        Args:\n",
    "            words (:obj:`list` of :obj:`str`, :obj:`str`): list of words.\n",
    "\n",
    "        Returns:\n",
    "            (:obj:`nltk.FreqDist`): Returns frequency dist.\n",
    "\n",
    "        \"\"\"\n",
    "        return FreqDist([w.lower() for w in words])\n",
    "    \n",
    "    def clean(self, words, display_top = 10, logging = True, exceptions = []):\n",
    "        \"\"\"Removes panctuations and stopwords from a corpus.\n",
    "            \n",
    "        Args:\n",
    "            words (:obj:`list` of :obj:`str`): The input corpus as list of words.\n",
    "            display_top (:obj:`int`, optional): Logging size (default 10).\n",
    "            logging (:obj:`bool`): Optional. When true stdout logging is done (default True).\n",
    "            exceptions (:obj:`list` of :obj:`str`, optional): The list terms that will not \n",
    "                be pruned (default None).\n",
    "\n",
    "        Returns:\n",
    "            (:obj:`nltk.FreqDist`): Returns the trimmed corpus as the NLTK obj.\n",
    "            exceptions (:obj:`list` of :obj:`str`): The exception list.\n",
    "\n",
    "        \"\"\"\n",
    "        def report(headline, fd):\n",
    "            \"\"\"Local method for logging the updates.\n",
    "            \n",
    "            Args:\n",
    "                headline (:obj:`str`): The state description\n",
    "\n",
    "            Returns:\n",
    "                logging (:obj:`bool`): True.\n",
    "            \"\"\"\n",
    "            print(headline)\n",
    "            print('Total term counts: {}'.format(sum(fd.values())))\n",
    "            pp.pprint(fd.most_common(display_top))\n",
    "            return True\n",
    "            \n",
    "        FD = TextCleaner.freq_dist(words)\n",
    "        headline = \"Initial state:\"\n",
    "        if logging: report(headline, FD)\n",
    "        \n",
    "        FD = self.remove_panctuation(FD, exceptions = exceptions)\n",
    "        headline = \"Removing panctuation only terms...\"\n",
    "        if logging: report(headline, FD)\n",
    "\n",
    "        FD = self.remove_stopwords(FD, exceptions = exceptions)\n",
    "        headline = \"Removing stopwords...\"\n",
    "        if logging: report(headline, FD)\n",
    "        return FD\n",
    "        \n",
    "    \n",
    "    def remove_panctuation(self, freq_dist, exceptions = []):\n",
    "        \"\"\"The static method removes punctuation only terms.\n",
    "            \n",
    "        Args:\n",
    "            freq_dist (:obj:`nltk.FreqDist`): list of words and more.\n",
    "            exceptions (:obj:`list` of :obj:`str`, optional): The exception list.\n",
    "\n",
    "        Returns:\n",
    "            (:obj:`nltk.FreqDist`): Returns frequency dist.\n",
    "\n",
    "        \"\"\"\n",
    "        for punctuation in tokenizer.CHARACTERS_TO_SPLIT:\n",
    "            if punctuation in exceptions: continue\n",
    "            dp = punctuation * 2\n",
    "            tp = dp + punctuation\n",
    "            if punctuation in freq_dist.keys():\n",
    "                del freq_dist[punctuation]\n",
    "            if dp in freq_dist.keys():\n",
    "                del freq_dist[dp]\n",
    "            if tp in freq_dist.keys():\n",
    "                del freq_dist[tp]\n",
    "        return freq_dist\n",
    "    \n",
    "    def remove_stopwords(self, freq_dist, exceptions = []):\n",
    "        \"\"\"The static method removes stopwords.\n",
    "            \n",
    "        Args:\n",
    "            freq_dist (:obj:`nltk.FreqDist`): list of words and more.\n",
    "            exceptions (:obj:`list` of :obj:`str`, optional): The exception list.\n",
    "\n",
    "        Returns:\n",
    "            (:obj:`nltk.FreqDist`): Returns frequency dist.\n",
    "\n",
    "        \"\"\"\n",
    "        for term in self.stopwords:\n",
    "            if self.isexception(term, exceptions): continue\n",
    "            if term in freq_dist.keys():\n",
    "                del freq_dist[term]\n",
    "        return freq_dist\n",
    "    \n",
    "    def remove_numerals(self, freq_dist, remove_any = False, exceptions = []):\n",
    "        \"\"\"The method removes terms with numeral literals.\n",
    "            \n",
    "        Note:\n",
    "            When remove_any is selected, literals such as 3D would vanish.\n",
    "            \n",
    "        Args:\n",
    "            freq_dist (:obj:`nltk.FreqDist`): list of words and more.\n",
    "            remove_any (:obj:`bool`, optional): If True mumeral and literal mixed terms are removed.\n",
    "            exceptions (:obj:`list` of :obj:`str`, optional): The exception list.\n",
    "\n",
    "        Returns:\n",
    "            (:obj:`nltk.FreqDist`): Returns frequency dist.\n",
    "\n",
    "        \"\"\"\n",
    "        freq_distN = copy.deepcopy(freq_dist)\n",
    "        if remove_any:\n",
    "            pattern_numbers = re.compile('[0-9]')\n",
    "            def has_numbers(x):\n",
    "                return(pattern_numbers.search(x) is not None)\n",
    "            for term in freq_dist.keys():\n",
    "                if self.isexception(term, exceptions, stemcheck=True): continue\n",
    "                if has_numbers(term):\n",
    "                    print(term)\n",
    "                    del freq_distN[term]\n",
    "        else:\n",
    "            pattern_letters = re.compile('[a-z]')\n",
    "            def has_letters(x):\n",
    "                return(pattern_letters.search(x) is not None)\n",
    "            for term in freq_dist.keys():\n",
    "                if self.isexception(term, exceptions, stemcheck=True): continue\n",
    "                if not has_letters(term):\n",
    "                    print(term)\n",
    "                    del freq_distN[term]\n",
    "        return freq_distN\n",
    "                \n",
    "    def remove_short_terms(self, freq_dist, threshold = 1, exceptions = []):\n",
    "        \"\"\"The method removes terms that are below a certain length.\n",
    "        \n",
    "        Args:\n",
    "            freq_dist (:obj:`nltk.FreqDist`): list of words and more.\n",
    "            threshold (:obj:`int`, optional): The charcter length of a term (default 1).\n",
    "            exceptions (:obj:`list` of :obj:`str`, optional): The exception list.\n",
    "\n",
    "        Returns:\n",
    "            (:obj:`nltk.FreqDist`): Returns frequency dist.\n",
    "\n",
    "        \"\"\"\n",
    "        freq_distN = copy.deepcopy(freq_dist)\n",
    "        for term in freq_dist.keys():\n",
    "            if self.isexception(term, exceptions): continue\n",
    "            if len(term) <= threshold:\n",
    "                del freq_distN[term]\n",
    "        return freq_distN\n",
    "\n",
    "    def remove_rare_terms(self, freq_dist, below = 3, exceptions = []):\n",
    "        \"\"\"The method removes terms that have rare occurances.\n",
    "        \n",
    "        Note:\n",
    "            Such removal may help reduce errenous and random terms.\n",
    "            \n",
    "        Args:\n",
    "            freq_dist (:obj:`nltk.FreqDist`): list of words and more.\n",
    "            below (:obj:`int`, optional): The minumum allowed frequency count (default 3).\n",
    "            exceptions (:obj:`list` of :obj:`str`, optional): The exception list.\n",
    "\n",
    "        Returns:\n",
    "            (:obj:`nltk.FreqDist`): Returns frequency dist.\n",
    "\n",
    "        \"\"\"\n",
    "        freq_distN = copy.deepcopy(freq_dist)\n",
    "        for term in freq_dist.keys():\n",
    "            if term in exceptions: continue\n",
    "            if freq_dist[term] < below:\n",
    "                del freq_distN[term]\n",
    "        return freq_distN\n",
    "    \n",
    "    \n",
    "    @staticmethod\n",
    "    def make_exceptions(exceptions, stemf = lambda x:x, stemcheck=False):\n",
    "        \"\"\"The static method makes the exception list and returns it.\n",
    "\n",
    "        Args:\n",
    "            exceptions (:obj:`list`): The list of exception terms.\n",
    "            stemf (x :obj:`str` -> y: :obj:`str`, optional): A stemmer function (default f(x) = x)\n",
    "            stemcheck (:obj:`bool`, optional): if the list to be extended via the stems (default False)\n",
    "            \n",
    "        Returns:\n",
    "            (:obj:`set`): The exception set. If stemcheck is opted both terms and their stems \n",
    "                is be represented in the list.\n",
    "\n",
    "        \"\"\"\n",
    "        exceptions = set(exceptions)\n",
    "        if stemcheck:\n",
    "            stems = [stemf(e) for e in exceptions if e]\n",
    "            exceptions.union(stems)\n",
    "        return exceptions\n",
    "        \n",
    "        \n",
    "    def isexception(self, term, exceptions=[], stemcheck=False):\n",
    "        \"\"\"The static makes the exception list and returns it.\n",
    "\n",
    "        Args:\n",
    "            term (:obj:`str`): The term.\n",
    "            exceptions (:obj:`list`, optional): The list of exception terms (default None).\n",
    "            stemcheck (:obj:`bool`, optional): if the list to be extended via the stems (default False)\n",
    "\n",
    "        Returns:\n",
    "            (:obj:`bool`): \n",
    "\n",
    "        \"\"\"\n",
    "        extended = self.exceptions.union(exceptions)\n",
    "        if stemcheck:\n",
    "            exceptions = self.make_exceptions(exceptions, self.stemf, stemcheck=True)\n",
    "        return term in exceptions\n",
    "    \n",
    "    def set_exceptions(self, exceptions, stemcheck=False):\n",
    "        \"\"\"Sets instance-wide exception set.\n",
    "\n",
    "        Args:\n",
    "            exceptions (:obj:`list` of `str`): The list of exception terms.\n",
    "            stemcheck (:obj:`bool`, optional): if the list to be extended via the stems (default False)\n",
    "\n",
    "        Returns:\n",
    "            (:obj:`bool`): True\n",
    "\n",
    "        \"\"\"\n",
    "        self.exceptions = self.make_exceptions(exceptions, self.stemf, stemcheck)\n",
    "        return True\n",
    "        \n",
    "    def remove_exceptions(self):\n",
    "        \"\"\"Removes instance's exception set.\n",
    "\n",
    "        Args:\n",
    "\n",
    "        Returns:\n",
    "            (:obj:`bool`): \n",
    "\n",
    "        \"\"\"\n",
    "        self.exceptions = set()\n",
    "        return True\n",
    "    \n",
    "    def remove_contains(self, literals, exceptions = []):\n",
    "        \"\"\"Removes the terms that contains the specific literals.\n",
    "\n",
    "        Note:\n",
    "            To be implemtnted\n",
    "        Args:\n",
    "\n",
    "        Returns:\n",
    "            \n",
    "\n",
    "        \"\"\"\n",
    "        pass\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    import matplotlib.pyplot as plt\n",
    "    import numpy as np\n",
    "    from sklearn import linear_model\n",
    "except:\n",
    "    raise ImportError(\"Import error at :obj:`Scoring`\")\n",
    "        \n",
    "class Scoring(object):\n",
    "    \"\"\"The object given term frequency distribution of a foreground specific corpus and a background\n",
    "    reference corpus, provides tools that help to compute specificity of each term in the foreground corpus.\n",
    "    \n",
    "    This kind of scoring is mainly to be used for the cases where an input text around a specific\n",
    "    theme or topic is given. The process expects a tokenized, cleaned text with term counts.\n",
    "    \n",
    "    Note:\n",
    "        It consumes a Corpus object and uses its methods and attributed and mutates it unless desired otherwise.\n",
    "\n",
    "    Attributes:\n",
    "        sCorpus (:obj:`Corpus`): A Corpus class instance of the specific corpus to be scored.\n",
    "        rCorpus (:obj:`Corpus`): A Corpus class instance of the reference corpus.\n",
    "        common (:obj:`list` of `str`): The common terms between the foreground and backgrouns corpus\n",
    "        distinct (:obj:`list` of `str`): The terms observed in the foreground but not in the backgrouns corpus\n",
    "        model: a prediction model created during instantiation process using the data of the class instance.\n",
    "            For details see`form_prediction_model` method description. \n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, sCorpus, rCorpus, mutate = False):\n",
    "        \"\"\"The class constructor.\n",
    "\n",
    "        Args:\n",
    "            sCorpus (:obj:`Corpus`): A Corpus class instance of the specific corpus to be scored.\n",
    "            rCorpus (:obj:`Corpus`): A Corpus class instance of the reference corpus.\n",
    "            mutate (:obj:`bool`, optional): A flag when true mutates the input Corpus object \n",
    "                for specific text corpus (default False).\n",
    "            \n",
    "        \"\"\"\n",
    "            \n",
    "        self.sCorpus = copy.deepcopy(sCorpus) if mutate else sCorpus\n",
    "        self.sCorpus.labels = dict()\n",
    "        self.sCorpus.scores = dict()\n",
    "        self.rCorpus = rCorpus\n",
    "        nt = self.sCorpus.get_count_uniques()\n",
    "        self.common = self.sCorpus.intersection(rCorpus)\n",
    "        print('{} / {}  raw term matches found.'.format(len(self.common), nt))\n",
    "        print('Identifying specificity score for each matched terms...')\n",
    "        self.compute_commons()\n",
    "        self.distinct = sCorpus.difference(rCorpus)\n",
    "        print('{} / {} non-raw match found.'.format(len(self.distinct), nt))\n",
    "        print('Identifying specifificity score for stem matches...')\n",
    "        self.compute_stem_commons()\n",
    "        print('Forming prediction model for non-matching terms...')\n",
    "        self.form_prediction_model()\n",
    "        print('Computing specificity score for frequent distinct terms...')\n",
    "        self.compute_distincts()\n",
    "        \n",
    "   \n",
    "    def compute_commons(self):\n",
    "        \"\"\"Computes the specifity score of the terms in the corpus.\n",
    "        \n",
    "        Note:\n",
    "            It is a simple log likelihood measure. It compares frequency count of a term in\n",
    "            a specific corpus versus its frequency count in the reference reference corpus.\n",
    "            Here assumption is that the reference corpus is a large enough sample of the language\n",
    "            at observing the occurance of a term. Then having a higher/lower observation frequency of\n",
    "            a term in the specific corpus is a proxy indicator for the term choice while having a debate\n",
    "            on the topic.\n",
    "\n",
    "            The likelihood ratio for a term P_t is calculated as:\n",
    "            .. math::\n",
    "                P_t = log ( (ntS/NS) / (ntR/NR) )\n",
    "            \n",
    "            where\n",
    "                - *ntS* is the raw frequency count of the term in the entire specific corpus\n",
    "                - *ntR* is the raw frequenccy count of the term in the reference corpus\n",
    "                - *NS* is the total number of terms in the specific corpus\n",
    "                - *NR* is the total number of terms in the reference corpus\n",
    "            \n",
    "            It should be noted that frequency counts are calculated after having applied the same tokenization\n",
    "            and post processing such as excluding stop-words, pancuations, rare terms, etc both on the reference\n",
    "            corpus and the specific corpus.\n",
    "            \n",
    "        Args:\n",
    "            None\n",
    "            \n",
    "        Returns:\n",
    "            (:obj:`bool`): Notifying completion of scoring.\n",
    "            \n",
    "        \"\"\"\n",
    "        for w in self.common:\n",
    "            s = log(self.sCorpus.tf_dist.freq(w) / self.rCorpus.tf_dist.freq(w))\n",
    "            self.sCorpus.scores[w] = s\n",
    "            self.sCorpus.ref[w] = self.rCorpus.tf_dist.freq(w)\n",
    "            self.sCorpus.labels[w] = 'raw'\n",
    "        return True\n",
    "    \n",
    "    def compute_stem_commons(self):\n",
    "        \"\"\"Computes the specifity score of the terms in the corpus when the term as it is not \n",
    "            matched by a term in the reference corpus. It matches the stems. The loglikelihood\n",
    "            ration is applied over the mean frequency counts of the matching stems.\n",
    "            \n",
    "            See `compute_commons` method description for details.\n",
    "            \n",
    "        Args:\n",
    "            None\n",
    "        Returns:\n",
    "            (:obj:`bool`): Notifying completion of scoring.\n",
    "            \n",
    "        \"\"\"\n",
    "        matched = []\n",
    "        for w in self.distinct:\n",
    "            stem = self.sCorpus.stems[w]\n",
    "            matches = [self.rCorpus.tf_dist.freq(w) for w,s in self.rCorpus.stems.items() if s == stem]\n",
    "            if not matches: continue\n",
    "            matched.append(w)\n",
    "            meanw = np.mean(matches)\n",
    "            s = log(self.sCorpus.tf_dist.freq(w) / meanw)\n",
    "            self.sCorpus.scores[w] = s\n",
    "            self.sCorpus.labels[w] = 'stem' \n",
    "            self.sCorpus.ref[w] = meanw\n",
    "        print('{} stems matched.'.format(len(matched)))\n",
    "        self.distinct = list(set(self.distinct).difference(set(matched)))\n",
    "        return True\n",
    "    \n",
    "    \n",
    "    def compute_distincts(self):\n",
    "        \"\"\"Computes the specifity score of the terms in the corpus when neither the term nor its stems\n",
    "            matched by the background corpus.\n",
    "            \n",
    "        Note:\n",
    "            It uses a log linear regression model to predict likelihood of the dictinct terms.\n",
    "            The model is trained using the scores and frequencies within the matching set.\n",
    "            \n",
    "            See `form_prediction_model` method description for details.\n",
    "            \n",
    "        Args:\n",
    "            None\n",
    "        Returns:\n",
    "            (:obj:`bool`): Notifying completion of scoring.\n",
    "            \n",
    "        \"\"\"\n",
    "        for w in self.distinct:\n",
    "            f = self.sCorpus.tf_dist[w]\n",
    "            s = self.predict(w,f)\n",
    "            self.sCorpus.scores[w] = s\n",
    "            self.sCorpus.ref[w] = None\n",
    "            self.sCorpus.labels[w] = 'noref'\n",
    "        return True\n",
    "    \n",
    "        \n",
    "    def form_prediction_model(self, threshold = 1.0):\n",
    "        \"\"\"The method creats the prediction model to be used for distinct terms.\n",
    "            \n",
    "        Note:\n",
    "            It is based on a log-linear regression. The model is created using the observed\n",
    "            scores and frequencies within the matching set. The model aims to fit a best line\n",
    "            to logarithm of the observed term frequencies vs associated scores.\n",
    "            \n",
    "            Considering the fact that frequent distinct terms are likely among the ones with a \n",
    "            higher specificity, the terms with relatively high scores are used for the regression.\n",
    "            The R-squared of the regression tests have been used for validation of the approach.\n",
    "            In the same reasoning among the all distinct terms the ones with relatively higher frequencies\n",
    "            are considered for scoring.\n",
    "            \n",
    "        ToDo:\n",
    "            The model training to be improved considering terms with relatively high term frequencies\n",
    "            and high specificity scores. Observe the scatter plots for the insight.\n",
    "            \n",
    "        Args:\n",
    "            threshold (:obj:`float`, optional):  The default value is driven from regression tests on \n",
    "                test cases (default 1.0).\n",
    "                \n",
    "        Returns:\n",
    "            (:obj:`bool`): Notifying completion of scoring.\n",
    "            \n",
    "        \"\"\"\n",
    "        x = list()\n",
    "        y = list()\n",
    "        for w in self.sCorpus.scores.keys():\n",
    "            s = self.sCorpus.scores[w]\n",
    "            if not s: continue\n",
    "            if s < threshold: continue\n",
    "            y.append(s)\n",
    "            x.append(log(self.sCorpus.tf_dist[w]))\n",
    "        plt.scatter(x,y)\n",
    "        plt.show()\n",
    "        xa = np.asarray(x)\n",
    "        lm = linear_model.LinearRegression()\n",
    "        self.model = lm.fit(xa.reshape(-1,1),y)\n",
    "        return True\n",
    "    \n",
    "\n",
    "    def predict(self, w, count, minp = 0.001, minf = 3):\n",
    "        \"\"\"The method assigns a predicted score to a given term with a a frequency\n",
    "            over the designated threshold. An internally formed prediction model is used.\n",
    "            The natural logorithm of raw frequency counts is passed to the model. See \n",
    "            `form_prediction_model` method description for details.\n",
    "            \n",
    "        Args:\n",
    "            count (:obj:`int`): The raw frequency count.\n",
    "            minp (:obj:`float`, optional): The relative frequency threshold (default 0.001).\n",
    "            minf (:obj:`int`, optional): The raw frequency threshold (default 3).\n",
    "                \n",
    "        Returns:\n",
    "            (:obj:`float`): The predicted score.\n",
    "            \n",
    "        \"\"\"\n",
    "        if count < minf: return None\n",
    "        if self.sCorpus.tf_dist.freq(w) < minp: return None\n",
    "        return self.model.predict(log(count))[0]\n",
    "    \n",
    "    def get_scores_by(self, stype='raw'):\n",
    "        \"\"\"The method returns computed/available scores by the label of the terms.\n",
    "        \n",
    "        Note:\n",
    "            The labels in this implementation correspond:\n",
    "            - raw: the term as it is was identified in the background corpus, so\n",
    "                a loglikelihood scoring was applied\n",
    "            - stem: not the term as it is but its stem was identified, so mean of the observed\n",
    "                stem occurances in the background was used as the reference\n",
    "            - noref: neither the term nor its stem was identified, so the prediction model was used\n",
    "                for the frequent ones.\n",
    "            \n",
    "        Args:\n",
    "            stype (:obj:`str`, optional): The term scoring type (default 'raw').\n",
    "                \n",
    "        Returns:\n",
    "            (:obj:`dict`): The term scores.\n",
    "            \n",
    "        \"\"\"\n",
    "        return {w:self.sCorpus.scores[w] for w,t in self.sCorpus.labels.items() if t == stype}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WikiCorpus(object):\n",
    "    \"\"\"The object contains a set of tools to process the set of \n",
    "        documents generated collected by the wiki crawler.\n",
    "\n",
    "    Attributes:\n",
    "        collection_json (:obj:`str`): This is a filename to the scraped data.\n",
    "            Each JSON document is expected to have following fields:\n",
    "            - theme: Topic identifier, ex: Sustainability\n",
    "            - theme.id: A unique category identifier\n",
    "            - document.id: A unique document id\n",
    "            - title: Title of the document\n",
    "            - url: Full URL of the document\n",
    "            - depth: The link distance from the seed docuement. The seed documents depth is 0.\n",
    "            - text: The string data scraped from the page without tags. Pancuations are not \n",
    "                required but terms are expected to be delineated by white space. \n",
    "        collection (:obj:`list` of :obj:`dict`): Loaded json file into native list of dictionaries.\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, collection):\n",
    "        \"\"\"The class constructor.\n",
    "\n",
    "        Args:\n",
    "            collection (:obj:`str`): A filename to a previously scraped data.\n",
    "\n",
    "        \"\"\"\n",
    "        self.collection_json = collection\n",
    "        self.load_corpus()\n",
    "        \n",
    "        \n",
    "    def load_corpus(self, collection = None):\n",
    "        \"\"\"The method loads imports json file into a native collection.\n",
    "          \n",
    "        Args:\n",
    "            collection (:obj:`str`): A filename to a previously scraped data.\n",
    "                \n",
    "        Returns:\n",
    "            (:obj:`bool`): True.\n",
    "            \n",
    "        \"\"\"\n",
    "            \n",
    "        if collection:\n",
    "            self.collection_json = collection\n",
    "        with open(self.collection_json, \"r\") as f:\n",
    "            _text = f.read()\n",
    "        self.collection = json.loads(_text)\n",
    "        print('Number of documents in the corpus: {}\\n'.format(len(self.collection)))\n",
    "        return True\n",
    "    \n",
    "    \n",
    "    def get_document_fields(self):\n",
    "        \"\"\"The method lists the fields of each json field of its collection.\n",
    "          \n",
    "        Args:\n",
    "            None  \n",
    "            \n",
    "        Returns:\n",
    "            (:obj:`dict_keys`): List of keys.\n",
    "            None: When the collection is empty.\n",
    "            \n",
    "        \"\"\"\n",
    "        msg = '{} must have been a non-empty list of dict objects'.format(self.collection)\n",
    "        assert isinstance(self.collection, list), msg\n",
    "        if not len(self.collection): return\n",
    "        return self.collection[0].keys()\n",
    "    \n",
    "    \n",
    "    def list_themes(self):\n",
    "        \"\"\"The method lists the summary of themes/topics in the collection.\n",
    "          \n",
    "        Args:\n",
    "            None  \n",
    "            \n",
    "        Returns:\n",
    "            (:obj:`list` of :obj:`dict`): List of dictionry where keys are\n",
    "                - name: theme's textual descriptor\n",
    "                - id: theme's unique id\n",
    "                - count: number of articles under the theme.   \n",
    "        \n",
    "        \"\"\"\n",
    "        themes = dict()\n",
    "        count = 0\n",
    "        for article in self.collection:\n",
    "            theme_id = article['theme.id']\n",
    "            if theme_id not in themes.keys():\n",
    "                themes[theme_id] = (article['theme'], 0)\n",
    "            else:\n",
    "                t,c = themes[theme_id]\n",
    "                c += 1\n",
    "                themes[theme_id] = (t, c)\n",
    "        themes = [{'id':k, 'name':v[0], 'num_of_articles':v[1]} for k,v in themes.items()]\n",
    "        return themes\n",
    "\n",
    "    \n",
    "    def get_theme_title(self, theme_id):\n",
    "        \"\"\"The method returns topic name.\n",
    "          \n",
    "        Args:\n",
    "            theme_id (:obj:`int`): The unique theme id. \n",
    "            \n",
    "        Returns:\n",
    "            ( :obj:`str`): Topic name.  \n",
    "        \n",
    "        \"\"\"\n",
    "        title = None\n",
    "        for article in self.collection:\n",
    "            if article['theme.id'] == theme_id:\n",
    "                title = article['theme']\n",
    "                break\n",
    "        return title\n",
    "\n",
    "    def get_theme_id(self, theme_name):\n",
    "        \"\"\"The method returns topic id of the first match theme name.\n",
    "          \n",
    "        Args:\n",
    "            theme_name (:obj:`str`): The theme or topic name. \n",
    "            \n",
    "        Returns:\n",
    "            ( :obj:`int`): A unique theme identifier.  \n",
    "        \n",
    "        \"\"\"\n",
    "        theme_id = None\n",
    "        for article in self.collection:\n",
    "            if article['theme'] == theme_name:\n",
    "                theme_id = article['theme.id']\n",
    "                break\n",
    "        return int(theme_id)\n",
    "\n",
    "    \n",
    "    def display_documents_list(self, tid=None, stdout = True):\n",
    "        \"\"\"List the articles meta data and crawling information on them.\n",
    "        \n",
    "        Args:\n",
    "            tid (:obj:`int`, optional): Used if documents info under a specific theme is desired \n",
    "                otherwise a summary of whole set is returned or displayed (default None).\n",
    "            stdout (:obj:`bool`, optional): Whether the info is to be displayed/printed\n",
    "                to standard io (default True).\n",
    "                \n",
    "        Returns:\n",
    "            ( :obj:`list`): A summary list of documents in the collection. \n",
    "            \n",
    "\n",
    "        \"\"\"\n",
    "        if stdout: print('document.id :: theme.id :: theme :: depth :: url')\n",
    "        articles = copy.deepcopy(self.collection)\n",
    "        for a in articles:\n",
    "            if 'text' in a:\n",
    "                del a['text']\n",
    "        if tid:\n",
    "            articles = [a for a in articles if a['theme.id'] == tid]   \n",
    "        articles = sorted(articles, key=lambda x: (x['theme.id'], x['depth']))\n",
    "        if not stdout: return articles\n",
    "        for a in articles:\n",
    "            print(a['document.id'],a['theme.id'],a['theme'],a['depth'],a['url'])\n",
    "        return articles\n",
    "    \n",
    "\n",
    "    def prune(self, themes_to_keep = [], docs_to_drop = [], istodrop = lambda x: False):\n",
    "        \"\"\"The method is used to filter out documents from the set.\n",
    "            The order of prunning is as follows:\n",
    "            - when a none empty list is provide all the documents not belonging themes to be kept\n",
    "                are prunned entirely. Note that when initial list is empty it doesn't have an effect.\n",
    "            - of remaing documents those appear in docs_to_drop are prunned\n",
    "            - of the remaing docs those produce a True at a call on the predicate function are dropped.\n",
    "            \n",
    "            The function can be repeatedly called until a desired level of prunning is achieved.\n",
    "        \n",
    "        Args:\n",
    "            themes_to_keep (:obj:`list` of :obj:`int`, optional): The list of theme ids to be kept (default Empty).\n",
    "            docs_to_drop (:obj:`list`, optional): The list of doc ids to be dropt (default Empty). \n",
    "            f (x :obj:`dict_item` -> :obj:`bool`, optional): A predicate function (default lambda x:False)\n",
    "            \n",
    "        Returns:\n",
    "            (:obj:`bool`): True. \n",
    "\n",
    "        \"\"\"\n",
    "        if themes_to_keep:\n",
    "            themes = list(map(lambda x: int(x), themes_to_keep))\n",
    "            self.collection = [a for a in self.collection if int(a['theme.id']) in themes]\n",
    "        self.collection = [a for a in self.collection if a['document.id'] not in docs_to_drop]\n",
    "        self.collection = [a for a in self.collection if not istodrop(a)]\n",
    "        return True\n",
    "\n",
    "\n",
    "    def collate(self, by_theme_id = None, by_doc_ids = [], marker = \"\\n\"):\n",
    "        \"\"\"The method collects the desired set of documents concatenates them creating a unified document.\n",
    "            The order of merge is as follows:\n",
    "            - If a theme is given then all the documents under that theme are to be joined.\n",
    "            - When a list of docs is given, only those in the list are kept.\n",
    "            Note that if no specified condition is passed then all the documents in the collection,\n",
    "            those presumably after a prunning session, are collated.\n",
    "        \n",
    "        Args:\n",
    "            by_theme_id (:obj:`int`, optional): The theme id of the docs to be collated.\n",
    "            by_doc_ids (:obj:`list` of :obj:`int`, optional): The list of doc ids to be collated (default Empty). \n",
    "            marker (:obj:`str`, optional): A delimiter (default newline)\n",
    "            \n",
    "        Returns:\n",
    "            (:obj:`str`): The collated text. \n",
    "        \n",
    "        \"\"\"\n",
    "        docs = self.collection\n",
    "        if by_theme_id:\n",
    "            docs = [d for d in docs if d['theme.id'] == by_theme_id] \n",
    "        if by_doc_ids:\n",
    "            docs = [d for d in docs if d['document.id'] in by_doc_ids]\n",
    "        texts = [d['text'] for d in docs]\n",
    "        text = marker.join(texts)\n",
    "        return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Preparing a crawled Wikipedia collection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Importing and a examining a crawled articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "JSON_FNAME = \"data/corpuses/schwartz.json\"\n",
    "TOPIC_NAME = 'stimulation'\n",
    "TOPIC_ID = 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents in the corpus: 563\n",
      "\n",
      "dict_keys(['theme', 'theme.id', 'document.id', 'title', 'url', 'depth', 'text'])\n",
      "[{'id': 1, 'name': 'universalism', 'num_of_articles': 205},\n",
      " {'id': 2, 'name': 'hedonism', 'num_of_articles': 85},\n",
      " {'id': 3, 'name': 'achievement', 'num_of_articles': 46},\n",
      " {'id': 4, 'name': 'power', 'num_of_articles': 24},\n",
      " {'id': 5, 'name': 'self-direction', 'num_of_articles': 37},\n",
      " {'id': 6, 'name': 'benevolence', 'num_of_articles': 57},\n",
      " {'id': 7, 'name': 'conformity', 'num_of_articles': 42},\n",
      " {'id': 8, 'name': 'tradition', 'num_of_articles': 18},\n",
      " {'id': 9, 'name': 'stimulation', 'num_of_articles': 7},\n",
      " {'id': 10, 'name': 'security', 'num_of_articles': 32}]\n",
      "9\n",
      "stimulation\n"
     ]
    }
   ],
   "source": [
    "# Loading:\n",
    "WikiC = WikiCorpus(JSON_FNAME)\n",
    "# Getting fields name on each document in the collection\n",
    "print(WikiC.get_document_fields())\n",
    "pp.pprint(WikiC.list_themes())\n",
    "print(WikiC.get_theme_id(TOPIC_NAME))\n",
    "print(WikiC.get_theme_title(TOPIC_ID))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 2 Filtering out by a set of use case creteria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function prune in module __main__:\n",
      "\n",
      "prune(self, themes_to_keep=[], docs_to_drop=[], istodrop=<function WikiCorpus.<lambda> at 0x11c086950>)\n",
      "    The method is used to filter out documents from the set.\n",
      "        The order of prunning is as follows:\n",
      "        - when a none empty list is provide all the documents not belonging themes to be kept\n",
      "            are prunned entirely. Note that when initial list is empty it doesn't have an effect.\n",
      "        - of remaing documents those appear in docs_to_drop are prunned\n",
      "        - of the remaing docs those produce a True at a call on the predicate function are dropped.\n",
      "        \n",
      "        The function can be repeatedly called until a desired level of prunning is achieved.\n",
      "    \n",
      "    Args:\n",
      "        themes_to_keep (:obj:`list` of :obj:`int`, optional): The list of theme ids to be kept (default Empty).\n",
      "        docs_to_drop (:obj:`list`, optional): The list of doc ids to be dropt (default Empty). \n",
      "        f (x :obj:`dict_item` -> :obj:`bool`, optional): A predicate function (default lambda x:False)\n",
      "        \n",
      "    Returns:\n",
      "        (:obj:`bool`): True.\n",
      "\n",
      "document.id :: theme.id :: theme :: depth :: url\n",
      "528 9 stimulation 1 https://en.wikipedia.org/wiki/Tourism\n",
      "530 9 stimulation 1 https://en.wikipedia.org/wiki/Travel\n",
      "document.id :: theme.id :: theme :: depth :: url\n",
      "528 9 stimulation 1 https://en.wikipedia.org/wiki/Tourism\n",
      "530 9 stimulation 1 https://en.wikipedia.org/wiki/Travel\n",
      "document.id :: theme.id :: theme :: depth :: url\n",
      "528 9 stimulation 1 https://en.wikipedia.org/wiki/Tourism\n",
      "530 9 stimulation 1 https://en.wikipedia.org/wiki/Travel\n",
      "document.id :: theme.id :: theme :: depth :: url\n",
      "528 9 stimulation 1 https://en.wikipedia.org/wiki/Tourism\n",
      "530 9 stimulation 1 https://en.wikipedia.org/wiki/Travel\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'depth': 1,\n",
       "  'document.id': 528,\n",
       "  'theme': 'stimulation',\n",
       "  'theme.id': 9,\n",
       "  'title': 'Tourism',\n",
       "  'url': 'https://en.wikipedia.org/wiki/Tourism'},\n",
       " {'depth': 1,\n",
       "  'document.id': 530,\n",
       "  'theme': 'stimulation',\n",
       "  'theme.id': 9,\n",
       "  'title': 'Travel',\n",
       "  'url': 'https://en.wikipedia.org/wiki/Travel'}]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "help(WikiCorpus.prune)\n",
    "WikiC.display_documents_list(tid=TOPIC_ID, stdout = True)\n",
    "WikiC.prune(themes_to_keep = [TOPIC_ID],\n",
    "            docs_to_drop = [523,535],\n",
    "            istodrop = lambda x: not 0 <= x['depth'] < 2)\n",
    "WikiC.display_documents_list()\n",
    "WikiC.prune(docs_to_drop = [531,533])\n",
    "WikiC.display_documents_list()\n",
    "WikiC.prune(istodrop = lambda x: len(x['text']) < 1000 or x['depth'] == 0 )\n",
    "WikiC.display_documents_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Joining a subset of articles within a topic for further analysis.\n",
    "\n",
    "It should be noted such document unification is a convenience if tokenization, frequency counts etc are done around a single topic. In other words, if the corpus holds a large set of documents and $tf$ $x$ $idf$ style examinations are intended, then merging the documents may cause loss of information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function collate in module __main__:\n",
      "\n",
      "collate(self, by_theme_id=None, by_doc_ids=[], marker='\\n')\n",
      "    The method collects the desired set of documents concatenates them creating a unified document.\n",
      "        The order of merge is as follows:\n",
      "        - If a theme is given then all the documents under that theme are to be joined.\n",
      "        - When a list of docs is given, only those in the list are kept.\n",
      "        Note that if no specified condition is passed then all the documents in the collection,\n",
      "        those presumably after a prunning session, are collated.\n",
      "    \n",
      "    Args:\n",
      "        by_theme_id (:obj:`int`, optional): The theme id of the docs to be collated.\n",
      "        by_doc_ids (:obj:`list` of :obj:`int`, optional): The list of doc ids to be collated (default Empty). \n",
      "        marker (:obj:`str`, optional): A delimiter (default newline)\n",
      "        \n",
      "    Returns:\n",
      "        (:obj:`str`): The collated text.\n",
      "\n",
      "Length of the corpus in terms of characters: 52792\n",
      "Length of the corpus in terms of characters: 52792\n",
      "Length of the corpus in terms of characters: 45194\n"
     ]
    }
   ],
   "source": [
    "help(WikiCorpus.collate)\n",
    "TEXT = WikiC.collate(by_theme_id = TOPIC_ID, marker = \" \")\n",
    "print('Length of the corpus in terms of characters: {}'.format(len(TEXT)))\n",
    "TEXT = WikiC.collate(by_doc_ids = [528,530], marker = \" \")\n",
    "print('Length of the corpus in terms of characters: {}'.format(len(TEXT)))\n",
    "TEXT = WikiC.collate(by_theme_id = TOPIC_ID, by_doc_ids = [528])\n",
    "print('Length of the corpus in terms of characters: {}'.format(len(TEXT)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Tokenizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "SPECIFIC_CORPUS = tokenizer.tokenize_strip_non_words(TEXT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Term counts: {} Corpus = {}, Ref Corpus = {}'.format(CURRENT_THEME.capitalize(), len(SPECIFIC_CORPUS), len(REFERENCE_CORPUS)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_FNAME_PREFIX = format_output_fname(CURRENT_THEME)\n",
    "print('The output file prefix: {}'.format(OUTPUT_FNAME_PREFIX))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Cleaning and counting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Forming stopwords list\n",
    "A domain specific stop-word is added to standard stopwords. Domain specific stopword needs to be extended through iterations or via state of art computational methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "STOPWORDS_STANDARD = \"data/stopwords_standard.txt\"\n",
    "STOPWORDS_SPECIFIC = \"data/stopwords_openmaker.txt\"\n",
    "SPECIFIC_TERMS = load_from_file(\"data/specifics_openmaker.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C = TextCleaner()\n",
    "C.load_stopwords(STOPWORDS_STANDARD)\n",
    "C.extend_stopwords(STOPWORDS_SPECIFIC)\n",
    "C.make_exceptions(SPECIFIC_TERMS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pp.pprint(C.stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Removing panctuations and stopwords "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tf_S = C.clean(SPECIFIC_CORPUS,logging = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tf_R = C.clean(REFERENCE_CORPUS,logging = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Cleaning the specific and reference corpora\n",
    "This is an example case for post proceesing in terms of cleaning. The pre-processing, that is data cleaning/preperation during or right after harvesting should be further improved to avoid such processes at this stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### 1.1 Loading a reference English language corpus\n",
    "#nltk.download()\n",
    "from nltk.corpus import brown\n",
    "REFERENCE_CORPUS = nltk.corpus.brown.words()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Removing all numeral terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tf_S.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Tf_Sn = C.remove_numerals(Tf_S, remove_any = False, exceptions = [])\n",
    "print(\"Corpus Specific: reduction due to all numeral terms = {}\".format(len(Tf_S) - len(Tf_Sn)))\n",
    "Tf_S = Tf_Sn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Tf_Rn = C.remove_numerals(Tf_R, remove_any = False, exceptions = [])\n",
    "print(\"Corpus Reference: Reduction due to all numeral terms = {}\".format(len(Tf_R) - len(Tf_Rn)))\n",
    "Tf_R = Tf_Rn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Removing terms that has numerals in them\n",
    "This would remove literals such as 20th, 3D, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Tf_Sn = C.remove_numerals(Tf_S, remove_any = True, exceptions = [])\n",
    "print(\"Corpus Specific: reduction due to partially numeral terms = {}\".format(len(Tf_S) - len(Tf_Sn)))\n",
    "Tf_S = Tf_Sn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Tf_Rn = C.remove_numerals(Tf_R, remove_any = True)\n",
    "print(\"Corpus Reference: Reduction due to partially numeral terms = {}\".format(len(Tf_R) - len(Tf_Rn)))\n",
    "Tf_R = Tf_Rn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Removing very short terms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tf_Sn = C.remove_short_terms(Tf_S, threshold = 1, exceptions = [])\n",
    "print(\"Corpus Specific: reduction due to short terms = {}\".format(len(Tf_S) - len(Tf_Sn)))\n",
    "Tf_S = Tf_Sn\n",
    "Tf_S.most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tf_Rn = C.remove_short_terms(Tf_R, threshold = 1)\n",
    "print(\"Corpus Reference: reduction due to short terms = {}\".format(len(Tf_R) - len(Tf_Rn)))\n",
    "Tf_R = Tf_Rn\n",
    "Tf_R.most_common(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Removing terms with rare frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tf_Sn = C.remove_rare_terms(Tf_S, below = 3)\n",
    "print(\"Corpus Specific: reduction due to rare terms = {}\".format(len(Tf_S) - len(Tf_Sn)))\n",
    "Tf_S = Tf_Sn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Tf_Rn = C.remove_rare_terms(Tf_R, below = 3)\n",
    "print(\"Corpus Reference: reduction due to rare terms = {}\".format(len(Tf_R) - len(Tf_Rn)))\n",
    "Tf_R = Tf_Rn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Anayzing and comparing corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "SC = Corpus(Tf_S)\n",
    "RC = Corpus(Tf_R)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SC.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SC.list_terms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RC.plot(20), SC.plot(20), SC.plot(20, cumulative = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SC.tabulate(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SC.get_top_frequents(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SC.get_least_frequents(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Number of unique words after cleaning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_Spec = SC.get_count_uniques()\n",
    "n_Ref = RC.get_count_uniques()\n",
    "print('Number of unique words: Specific = {}, Reference = {}'.format(n_Spec, n_Ref))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2  Total word counts after cleaning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_Spec = SC.get_size()\n",
    "s_Ref =  RC.get_size()\n",
    "print('Total of words: Specific = {}, Reference = {}'.format(s_Spec, s_Ref))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Displaying ordered list of terms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "List of words in the corpus in case, for a visual inspection. Such inspections will be used both to improve tokenization as well as filtering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SC.get_top_frequents(n_Spec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4 Set of terms/words that occure in both corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "common_words = SC.intersection(RC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(common_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.5 Set of terms/words that occur in the sample but not in the reference corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This specific set will be incorporated later below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_specifics = SC.difference(RC)\n",
    "print(len(input_specifics))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#pp.pprint(input_specifics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SC.set_specific_set(SPECIFIC_TERMS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()\n",
    "\n",
    "SC.set_stemmer(stemmer.stem)\n",
    "SC.compute_stems()\n",
    "\n",
    "RC.set_stemmer(stemmer.stem)\n",
    "RC.compute_stems()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "SC.labels = []\n",
    "ref_terms = RC.list_terms()\n",
    "labels = SC.label(lambda x: 'raw' if x in ref_terms else None)\n",
    "ref_stems = RC.stems.values()\n",
    "labels = SC.label(lambda x: 'stem' if SC.stemmer(x) in ref_stems else None, SC.labels)\n",
    "\n",
    "specifics = SC.specifics\n",
    "labels = SC.label(lambda x: 'om' if x in specifics else None, SC.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp.pprint({w:lab for w,lab in SC.labels.items() if lab == 'om'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len([w for w,lab in SC.labels.items() if not lab])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1 Identfying matching stems within the reference corpus. \n",
    "\n",
    "Note that the frequency counts are transferred accordingly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Handling input specific term set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2 Removing open-maker specific terms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.3 Remaining frequent input specifics\n",
    "The manual checking can help to determine what should go into \"specifics_openmaker.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Computing representation power of common words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S = Scoring(SC,RC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S.get_scores_by('noref')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "SCored = copy.deepcopy(S.sCorpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "DF = SCored.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(S.distinct)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Tabulating the results and generating the output file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "OUTPUT_FOLDER = \"./output/\"\n",
    "csvfile_name = OUTPUT_FOLDER + \"schwartz_\" + OUTPUT_FNAME_PREFIX + \".csv\"\n",
    "with open(csvfile_name, 'w') as csvfile:\n",
    "    #thewriter = csv.writer(csvfile, delimiter=',')\n",
    "    #for k,v in sorted(makerness.items(), key=lambda x:x[1][0], reverse=True): thewriter.writerow([k,v[0],v[1]])\n",
    "    DF.to_csv(csvfile_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.1 Outputfile name for the theme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(csvfile_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.2 Selecting a specific range of scores\n",
    "\n",
    "Note that with the function below a specific slice between a mix and max value can be determined. Besides, the filterin can be applied to any column as long as its data type is a number.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_t = 4\n",
    "aslice = filter_pandas_rows(DF, min_t = min_t)\n",
    "reduced = '_min{}'.format(min_t)\n",
    "filtered_csvfile_name = OUTPUT_FOLDER + \"schwartz_\" + OUTPUT_FNAME_PREFIX + reduced + \".csv\"\n",
    "aslice.to_csv(filtered_csvfile_name)\n",
    "print(filtered_csvfile_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#%connect_info"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
